{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9599e2f3-6b9c-4578-9501-7d5c65df408a",
   "metadata": {},
   "source": [
    "Copyright (c) 2023 Habana Labs, Ltd. an Intel Company.\n",
    "\n",
    "#### Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "you may not use this file except in compliance with the License. You may obtain a copy of the License at https://www.apache.org/licenses/LICENSE-2.0 Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a8d33aa-8f5b-4de3-bdaa-b2bfac88e1e8",
   "metadata": {},
   "source": [
    "# Using Paramater Efficient Fine Tuning on Llama 2 with 7B Parameters on One Intel&reg; Gaudi&reg; 2 AI Accelerator\n",
    "This example will Fine Tune the Llama2-7B model using Parameter Efficient Fine Tuining (PEFT) and then run inference on a text prompt.  This will be using the Llama2 model with two task examples from the Optimum Habana library on the Hugging Face model repository.   The Optimum Habana library is optimized for Deep Learning training and inference on First-gen Gaudi and Gaudi2 and offers tasks such as text generation, language modeling, question answering and more. For all the examples and models, please refer to the [Optimum Habana GitHub](https://github.com/huggingface/optimum-habana#validated-models).\n",
    "\n",
    "This example will Fine Tune the Llama2-7B model using Parameter Efficient Fine Tuining (PEFT) on the timdettmers/openassistant-guanaco dataset using the Language-Modeling Task in Optimum Habana."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f337ed16-dcfa-424f-a46d-933880d12d04",
   "metadata": {},
   "source": [
    "### Parameter Efficient Fine Tuning with Low Rank Adaptation\n",
    "Parameter Efficient Fine Tuning is a strategy for adapting large pre-trained language models to specific tasks while minimizing computational and memory demands.   It aims to reduce the computational cost and memory requirements associated with fine-tuning large models while maintaining or even improving their performance.  It does so by adding a smaller task-specific layer, leveraging knowledge distillation, and often relying on few-shot learning, resulting in efficient yet effective models for various natural language understanding tasks.   PEFT starts with a pre-trained language model that has already learned a wide range of language understanding tasks from a large corpus of text data. These models are usually large and computationally expensive.   Instead of fine-tuning the entire pre-trained model, PEFT adds a task-specific layer or a few task-specific layers on top of the pre-trained model. These additional layers are relatively smaller and have fewer parameters compared to the base model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "404692f3-1266-4dcb-8885-ec6016aa5bef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/ufa813565e6c2cd47e0e35a43b155dd4/Gaudi-tutorials/PyTorch/Single_card_tutorials\n"
     ]
    }
   ],
   "source": [
    "%cd ~/Gaudi-tutorials/PyTorch/Single_card_tutorials"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29ad8bfa-90ae-4e32-963f-821b92ddab0e",
   "metadata": {},
   "source": [
    "### Model Setup: "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9240274a-dd9a-4ca9-a73d-b4ff956c343d",
   "metadata": {},
   "source": [
    "##### Install the Parameter Efficient Fine Tuning Library methods\n",
    "This is taking the PEFT method from the Hugging Face repository and will be used to help create the PEFT Fine Tuning with the Llama2 model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7d21efb3-978e-4585-915a-4c8a9ba9b064",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting peft==0.10.0\n",
      "  Using cached peft-0.10.0-py3-none-any.whl (199 kB)\n",
      "Requirement already satisfied: huggingface-hub>=0.17.0 in /home/ufa813565e6c2cd47e0e35a43b155dd4/.local/lib/python3.10/site-packages (from peft==0.10.0) (0.26.2)\n",
      "Collecting accelerate>=0.21.0\n",
      "  Using cached accelerate-1.1.1-py3-none-any.whl (333 kB)\n",
      "Requirement already satisfied: safetensors in /home/ufa813565e6c2cd47e0e35a43b155dd4/.local/lib/python3.10/site-packages (from peft==0.10.0) (0.4.5)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from peft==0.10.0) (1.23.5)\n",
      "Requirement already satisfied: transformers in /home/ufa813565e6c2cd47e0e35a43b155dd4/.local/lib/python3.10/site-packages (from peft==0.10.0) (4.46.2)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from peft==0.10.0) (24.1)\n",
      "Requirement already satisfied: torch>=1.13.0 in /usr/local/lib/python3.10/dist-packages (from peft==0.10.0) (2.2.0a0+git8964477)\n",
      "Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from peft==0.10.0) (6.0)\n",
      "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from peft==0.10.0) (6.0.0)\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from peft==0.10.0) (4.66.5)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.17.0->peft==0.10.0) (2024.9.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.17.0->peft==0.10.0) (4.12.2)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.17.0->peft==0.10.0) (3.16.1)\n",
      "Requirement already satisfied: requests in /usr/lib/python3/dist-packages (from huggingface-hub>=0.17.0->peft==0.10.0) (2.25.1)\n",
      "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->peft==0.10.0) (1.13.3)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->peft==0.10.0) (3.3)\n",
      "Requirement already satisfied: jinja2 in /usr/lib/python3/dist-packages (from torch>=1.13.0->peft==0.10.0) (3.0.3)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers->peft==0.10.0) (2023.5.5)\n",
      "Requirement already satisfied: tokenizers<0.21,>=0.20 in /home/ufa813565e6c2cd47e0e35a43b155dd4/.local/lib/python3.10/site-packages (from transformers->peft==0.10.0) (0.20.3)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.13.0->peft==0.10.0) (1.3.0)\n",
      "Installing collected packages: accelerate, peft\n",
      "Successfully installed accelerate-1.1.1 peft-0.10.0\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip available: \u001b[0m\u001b[31;49m22.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.3.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython3 -m pip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "!{sys.executable} -m pip install peft==0.10.0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d91f0f3e-8bf8-4c1c-adbb-3926b292e5ed",
   "metadata": {},
   "source": [
    "##### Install the Optimum-Habana Library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c8408759-937d-472a-bd00-e67142a90fbb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip available: \u001b[0m\u001b[31;49m22.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.3.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython3 -m pip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!{sys.executable} -m pip install -q optimum-habana==1.11.1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89b06c57-0d8c-4d40-85ae-1eea945a3ef9",
   "metadata": {},
   "source": [
    "##### Pull the Hugging Face Examples from GitHub\n",
    "These contain the working Hugging Face Task Examples that have been optimized for Gaudi.  For Fine Tuning, we'll use the language-modeling task. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "50af3c79-6641-47d7-a440-09eba2bd5765",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/ufa813565e6c2cd47e0e35a43b155dd4/Gaudi-tutorials/PyTorch/Single_card_tutorials\n"
     ]
    }
   ],
   "source": [
    "%cd ~/Gaudi-tutorials/PyTorch/Single_card_tutorials"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "202263c2-e32d-4a96-b04c-42edb601daeb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cloning into 'optimum-habana'...\n",
      "remote: Enumerating objects: 19102, done.\u001b[K\n",
      "remote: Counting objects: 100% (107/107), done.\u001b[K\n",
      "remote: Compressing objects: 100% (70/70), done.\u001b[K\n",
      "remote: Total 19102 (delta 54), reused 63 (delta 31), pack-reused 18995 (from 1)\u001b[K\n",
      "Receiving objects: 100% (19102/19102), 9.54 MiB | 16.56 MiB/s, done.\n",
      "Resolving deltas: 100% (13283/13283), done.\n",
      "Note: switching to '989484c60c22a217dd5f89d9d70a703162188b7f'.\n",
      "\n",
      "You are in 'detached HEAD' state. You can look around, make experimental\n",
      "changes and commit them, and you can discard any commits you make in this\n",
      "state without impacting any branches by switching back to a branch.\n",
      "\n",
      "If you want to create a new branch to retain commits you create, you may\n",
      "do so (now or later) by using -c with the switch command. Example:\n",
      "\n",
      "  git switch -c <new-branch-name>\n",
      "\n",
      "Or undo this operation with:\n",
      "\n",
      "  git switch -\n",
      "\n",
      "Turn off this advice by setting config variable advice.detachedHead to false\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!git clone -b v1.11.1 https://github.com/huggingface/optimum-habana.git"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11e369a6-ce17-40a2-8a52-d735b7140e09",
   "metadata": {},
   "source": [
    "##### Go to the Language Modeling Task and install the model specific requirements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "441de6d9-2b6f-4cf2-8bfd-664cec1c4998",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/ufa813565e6c2cd47e0e35a43b155dd4/Gaudi-tutorials/PyTorch/Single_card_tutorials/optimum-habana/examples/language-modeling\n"
     ]
    }
   ],
   "source": [
    "%cd ~/Gaudi-tutorials/PyTorch/Single_card_tutorials/optimum-habana/examples/language-modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b2bffb2c-88e2-46d8-b234-eb9828a51ec2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip available: \u001b[0m\u001b[31;49m22.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.3.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython3.10 -m pip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install -q -r requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88ed7d45",
   "metadata": {},
   "source": [
    "##### How to access and Use the Llama 2 model\n",
    "\n",
    "Use of the pretrained model is subject to compliance with third party licenses, including the “Llama 2 Community License Agreement” (LLAMAV2). For guidance on the intended use of the LLAMA2 model, what will be considered misuse and out-of-scope uses, who are the intended users and additional terms please review and read the instructions in this link https://ai.meta.com/llama/license/.\n",
    "Users bear sole liability and responsibility to follow and comply with any third party licenses, and Habana Labs disclaims and will bear no liability with respect to users’ use or compliance with third party licenses.\n",
    "\n",
    "To be able to run gated models like the Meta Llama model, you need the following: \n",
    "- Have a HuggingFace account\n",
    "- Agree to the terms of use of the model in its model card on the HF Hub\n",
    "- set a read token\n",
    "- Login to your account using the HF CLI: run huggingface-cli login before launching your script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2aa53882-c834-4ff3-8fc4-742579ee8cea",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!huggingface-cli login --token <your_token_here>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2d4c81e-c48d-45e0-93de-579e53995602",
   "metadata": {},
   "source": [
    "## Fine Tuning the model with PEFT and LoRA\n",
    "\n",
    "We'll now run the fine tuning with the PEFT method. Remember that the PEFT methods only fine-tune a small number of extra model parameters, thereby greatly decreasing the computational and storage costs. Recent State-of-the-Art PEFT techniques achieve performance comparable to that of full fine-tuning.\n",
    "\n",
    "##### Here's a summary of the command required to run the Fine Tuning, you'll run this in the next cell below. \n",
    "Note in this case the following: \n",
    "1. Using the language modeling with LoRA; `run_lora_clm.py`\n",
    "2. It's very efficient: only 0.06% of the total paramters are being fine tuned of the total 7B parameters.\n",
    "4. Only 3 epochs are needed for fine tuning, it takes less than 20 minutes to run with the openassisant-guanaco dataset.\n",
    "5. We are using a cached model on the Intel Tiber Cloud to reduce download time.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "59efbfa4-4669-434f-ab94-cf12bfdfa39e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/ufa813565e6c2cd47e0e35a43b155dd4/.local/lib/python3.10/site-packages/transformers/utils/generic.py:462: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
      "  _torch_pytree._register_pytree_node(\n",
      "/home/ufa813565e6c2cd47e0e35a43b155dd4/.local/lib/python3.10/site-packages/transformers/utils/generic.py:319: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
      "  _torch_pytree._register_pytree_node(\n",
      "/home/ufa813565e6c2cd47e0e35a43b155dd4/.local/lib/python3.10/site-packages/transformers/utils/generic.py:319: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
      "  _torch_pytree._register_pytree_node(\n",
      "11/14/2024 05:39:16 - WARNING - __main__ -   Process rank: 0, device: hpu, distributed training: True, 16-bits training: True\n",
      "11/14/2024 05:39:16 - INFO - __main__ -   Training/evaluation parameters GaudiTrainingArguments(\n",
      "_n_gpu=0,\n",
      "accelerator_config={'split_batches': False, 'dispatch_batches': None, 'even_batches': True, 'use_seedable_sampler': True},\n",
      "adafactor=False,\n",
      "adam_beta1=0.9,\n",
      "adam_beta2=0.999,\n",
      "adam_epsilon=1e-08,\n",
      "adjust_throughput=False,\n",
      "auto_find_batch_size=False,\n",
      "bf16=True,\n",
      "bf16_full_eval=False,\n",
      "data_seed=None,\n",
      "dataloader_drop_last=False,\n",
      "dataloader_num_workers=0,\n",
      "dataloader_persistent_workers=False,\n",
      "dataloader_pin_memory=True,\n",
      "dataloader_prefetch_factor=None,\n",
      "ddp_backend=hccl,\n",
      "ddp_broadcast_buffers=None,\n",
      "ddp_bucket_cap_mb=230,\n",
      "ddp_find_unused_parameters=False,\n",
      "ddp_timeout=1800,\n",
      "debug=[],\n",
      "deepspeed=None,\n",
      "disable_tensor_cache_hpu_graphs=False,\n",
      "disable_tqdm=False,\n",
      "dispatch_batches=None,\n",
      "distribution_strategy=ddp,\n",
      "do_eval=True,\n",
      "do_predict=False,\n",
      "do_train=True,\n",
      "eval_accumulation_steps=None,\n",
      "eval_delay=0,\n",
      "eval_steps=None,\n",
      "evaluation_strategy=no,\n",
      "fsdp=[],\n",
      "fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False},\n",
      "fsdp_min_num_params=0,\n",
      "fsdp_transformer_layer_cls_to_wrap=None,\n",
      "full_determinism=False,\n",
      "gaudi_config_name=None,\n",
      "gradient_accumulation_steps=1,\n",
      "gradient_checkpointing=False,\n",
      "gradient_checkpointing_kwargs=None,\n",
      "greater_is_better=None,\n",
      "group_by_length=False,\n",
      "half_precision_backend=hpu_amp,\n",
      "hub_always_push=False,\n",
      "hub_model_id=None,\n",
      "hub_private_repo=False,\n",
      "hub_strategy=every_save,\n",
      "hub_token=<HUB_TOKEN>,\n",
      "ignore_data_skip=False,\n",
      "ignore_eos=True,\n",
      "include_inputs_for_metrics=False,\n",
      "include_num_input_tokens_seen=False,\n",
      "include_tokens_per_second=False,\n",
      "jit_mode_eval=False,\n",
      "label_names=None,\n",
      "label_smoothing_factor=0.0,\n",
      "learning_rate=0.0001,\n",
      "length_column_name=length,\n",
      "load_best_model_at_end=False,\n",
      "local_rank=0,\n",
      "log_level=passive,\n",
      "log_level_replica=warning,\n",
      "log_on_each_node=True,\n",
      "logging_dir=/home/ufa813565e6c2cd47e0e35a43b155dd4/Gaudi-tutorials/PyTorch/Single_card_tutorials/model_lora_llama_single/runs/Nov14_05-39-14_idc-training-gaudi-compute-04,\n",
      "logging_first_step=False,\n",
      "logging_nan_inf_filter=False,\n",
      "logging_steps=1.0,\n",
      "logging_strategy=steps,\n",
      "lr_scheduler_kwargs={},\n",
      "lr_scheduler_type=constant,\n",
      "max_grad_norm=0.3,\n",
      "max_hpu_graphs=None,\n",
      "max_steps=-1,\n",
      "metric_for_best_model=None,\n",
      "neftune_noise_alpha=None,\n",
      "no_cuda=False,\n",
      "non_blocking_data_copy=False,\n",
      "num_train_epochs=3.0,\n",
      "optim=adamw_torch,\n",
      "optim_args=None,\n",
      "output_dir=/home/ufa813565e6c2cd47e0e35a43b155dd4/Gaudi-tutorials/PyTorch/Single_card_tutorials/model_lora_llama_single,\n",
      "overwrite_output_dir=True,\n",
      "past_index=-1,\n",
      "per_device_eval_batch_size=8,\n",
      "per_device_train_batch_size=16,\n",
      "pipelining_fwd_bwd=False,\n",
      "prediction_loss_only=False,\n",
      "profiling_record_shapes=True,\n",
      "profiling_steps=0,\n",
      "profiling_warmup_steps=0,\n",
      "push_to_hub=False,\n",
      "push_to_hub_model_id=None,\n",
      "push_to_hub_organization=None,\n",
      "push_to_hub_token=<PUSH_TO_HUB_TOKEN>,\n",
      "ray_scope=last,\n",
      "remove_unused_columns=True,\n",
      "report_to=[],\n",
      "resume_from_checkpoint=None,\n",
      "run_name=/home/ufa813565e6c2cd47e0e35a43b155dd4/Gaudi-tutorials/PyTorch/Single_card_tutorials/model_lora_llama_single,\n",
      "save_on_each_node=False,\n",
      "save_only_model=False,\n",
      "save_safetensors=True,\n",
      "save_steps=500,\n",
      "save_strategy=no,\n",
      "save_total_limit=None,\n",
      "seed=42,\n",
      "skip_memory_metrics=True,\n",
      "split_batches=None,\n",
      "throughput_warmup_steps=3,\n",
      "torch_compile=False,\n",
      "torch_compile_backend=None,\n",
      "torch_compile_mode=None,\n",
      "torchdynamo=None,\n",
      "use_cpu=False,\n",
      "use_habana=True,\n",
      "use_hpu_graphs=False,\n",
      "use_hpu_graphs_for_inference=False,\n",
      "use_hpu_graphs_for_training=False,\n",
      "use_ipex=False,\n",
      "use_lazy_mode=True,\n",
      "use_legacy_prediction_loop=False,\n",
      "use_mps_device=False,\n",
      "warmup_ratio=0.03,\n",
      "warmup_steps=0,\n",
      "weight_decay=0.0,\n",
      ")\n",
      "/home/ufa813565e6c2cd47e0e35a43b155dd4/.local/lib/python3.10/site-packages/huggingface_hub/file_download.py:797: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/ufa813565e6c2cd47e0e35a43b155dd4/.local/lib/python3.10/site-packages/huggingface_hub/utils/_http.py\", line 406, in hf_raise_for_status\n",
      "    response.raise_for_status()\n",
      "  File \"/home/ufa813565e6c2cd47e0e35a43b155dd4/.local/lib/python3.10/site-packages/requests/models.py\", line 1024, in raise_for_status\n",
      "    raise HTTPError(http_error_msg, response=self)\n",
      "requests.exceptions.HTTPError: 401 Client Error: Unauthorized for url: https://huggingface.co/meta-llama/Llama-2-7b-hf/resolve/main/config.json\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/ufa813565e6c2cd47e0e35a43b155dd4/.local/lib/python3.10/site-packages/transformers/utils/hub.py\", line 398, in cached_file\n",
      "    resolved_file = hf_hub_download(\n",
      "  File \"/home/ufa813565e6c2cd47e0e35a43b155dd4/.local/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py\", line 114, in _inner_fn\n",
      "    return fn(*args, **kwargs)\n",
      "  File \"/home/ufa813565e6c2cd47e0e35a43b155dd4/.local/lib/python3.10/site-packages/huggingface_hub/file_download.py\", line 862, in hf_hub_download\n",
      "    return _hf_hub_download_to_cache_dir(\n",
      "  File \"/home/ufa813565e6c2cd47e0e35a43b155dd4/.local/lib/python3.10/site-packages/huggingface_hub/file_download.py\", line 969, in _hf_hub_download_to_cache_dir\n",
      "    _raise_on_head_call_error(head_call_error, force_download, local_files_only)\n",
      "  File \"/home/ufa813565e6c2cd47e0e35a43b155dd4/.local/lib/python3.10/site-packages/huggingface_hub/file_download.py\", line 1484, in _raise_on_head_call_error\n",
      "    raise head_call_error\n",
      "  File \"/home/ufa813565e6c2cd47e0e35a43b155dd4/.local/lib/python3.10/site-packages/huggingface_hub/file_download.py\", line 1376, in _get_metadata_or_catch_error\n",
      "    metadata = get_hf_file_metadata(\n",
      "  File \"/home/ufa813565e6c2cd47e0e35a43b155dd4/.local/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py\", line 114, in _inner_fn\n",
      "    return fn(*args, **kwargs)\n",
      "  File \"/home/ufa813565e6c2cd47e0e35a43b155dd4/.local/lib/python3.10/site-packages/huggingface_hub/file_download.py\", line 1296, in get_hf_file_metadata\n",
      "    r = _request_wrapper(\n",
      "  File \"/home/ufa813565e6c2cd47e0e35a43b155dd4/.local/lib/python3.10/site-packages/huggingface_hub/file_download.py\", line 277, in _request_wrapper\n",
      "    response = _request_wrapper(\n",
      "  File \"/home/ufa813565e6c2cd47e0e35a43b155dd4/.local/lib/python3.10/site-packages/huggingface_hub/file_download.py\", line 301, in _request_wrapper\n",
      "    hf_raise_for_status(response)\n",
      "  File \"/home/ufa813565e6c2cd47e0e35a43b155dd4/.local/lib/python3.10/site-packages/huggingface_hub/utils/_http.py\", line 423, in hf_raise_for_status\n",
      "    raise _format(GatedRepoError, message, response) from e\n",
      "huggingface_hub.errors.GatedRepoError: 401 Client Error. (Request ID: Root=1-67358d04-4d2291116e7b5c3305b7be5f;a944b1ee-3152-4e5a-bd62-4f8288b3ea2b)\n",
      "\n",
      "Cannot access gated repo for url https://huggingface.co/meta-llama/Llama-2-7b-hf/resolve/main/config.json.\n",
      "Access to model meta-llama/Llama-2-7b-hf is restricted. You must have access to it and be authenticated to access it. Please log in.\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/ufa813565e6c2cd47e0e35a43b155dd4/Gaudi-tutorials/PyTorch/Single_card_tutorials/optimum-habana/examples/language-modeling/run_lora_clm.py\", line 754, in <module>\n",
      "    main()\n",
      "  File \"/home/ufa813565e6c2cd47e0e35a43b155dd4/Gaudi-tutorials/PyTorch/Single_card_tutorials/optimum-habana/examples/language-modeling/run_lora_clm.py\", line 407, in main\n",
      "    config = AutoConfig.from_pretrained(model_args.model_name_or_path, **config_kwargs)\n",
      "  File \"/home/ufa813565e6c2cd47e0e35a43b155dd4/.local/lib/python3.10/site-packages/transformers/models/auto/configuration_auto.py\", line 1111, in from_pretrained\n",
      "    config_dict, unused_kwargs = PretrainedConfig.get_config_dict(pretrained_model_name_or_path, **kwargs)\n",
      "  File \"/home/ufa813565e6c2cd47e0e35a43b155dd4/.local/lib/python3.10/site-packages/transformers/configuration_utils.py\", line 633, in get_config_dict\n",
      "    config_dict, kwargs = cls._get_config_dict(pretrained_model_name_or_path, **kwargs)\n",
      "  File \"/home/ufa813565e6c2cd47e0e35a43b155dd4/.local/lib/python3.10/site-packages/transformers/configuration_utils.py\", line 688, in _get_config_dict\n",
      "    resolved_config_file = cached_file(\n",
      "  File \"/home/ufa813565e6c2cd47e0e35a43b155dd4/.local/lib/python3.10/site-packages/transformers/utils/hub.py\", line 416, in cached_file\n",
      "    raise EnvironmentError(\n",
      "OSError: You are trying to access a gated repo.\n",
      "Make sure to have access to it at https://huggingface.co/meta-llama/Llama-2-7b-hf.\n",
      "401 Client Error. (Request ID: Root=1-67358d04-4d2291116e7b5c3305b7be5f;a944b1ee-3152-4e5a-bd62-4f8288b3ea2b)\n",
      "\n",
      "Cannot access gated repo for url https://huggingface.co/meta-llama/Llama-2-7b-hf/resolve/main/config.json.\n",
      "Access to model meta-llama/Llama-2-7b-hf is restricted. You must have access to it and be authenticated to access it. Please log in.\n"
     ]
    }
   ],
   "source": [
    "!python3 run_lora_clm.py \\\n",
    "    --overwrite_output_dir=True \\\n",
    "    --model_name_or_path huggyllama/llama-7b \\\n",
    "    --dataset_name timdettmers/openassistant-guanaco \\\n",
    "    --bf16 True \\\n",
    "    --output_dir ~/Gaudi-tutorials/PyTorch/Single_card_tutorials/model_lora_llama_single \\\n",
    "    --num_train_epochs 3 \\\n",
    "    --per_device_train_batch_size 16 \\\n",
    "    --evaluation_strategy \"no\" \\\n",
    "    --save_strategy \"no\" \\\n",
    "    --learning_rate 1e-4 \\\n",
    "    --warmup_ratio  0.03 \\\n",
    "    --lr_scheduler_type \"constant\" \\\n",
    "    --max_grad_norm  0.3 \\\n",
    "    --logging_steps 1 \\\n",
    "    --do_train \\\n",
    "    --do_eval \\\n",
    "    --use_habana \\\n",
    "    --use_lazy_mode \\\n",
    "    --throughput_warmup_steps 3 \\\n",
    "    --lora_rank=8 \\\n",
    "    --lora_alpha=16 \\\n",
    "    --lora_dropout=0.05 \\\n",
    "    --lora_target_modules \"q_proj\" \"v_proj\" \\\n",
    "    --dataset_concatenation \\\n",
    "    --report_to none \\\n",
    "    --max_seq_length 512 \\\n",
    "    --low_cpu_mem_usage True \\\n",
    "    --validation_split_percentage 4 \\\n",
    "    --adam_epsilon 1e-08"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "340f4aa5-5dc0-4662-bbca-a7a156be37f2",
   "metadata": {},
   "source": [
    "#### LoRA Fine Tuning Completed\n",
    "You will now see a \"model_lora_llama_single\" folder created which contains the PEFT model `adapter_model.bin` which will be used in the inference example below. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2a5b5cf-e4a4-4d67-888b-e3ddcdff1a5d",
   "metadata": {},
   "source": [
    "## Inference with Llama 2\n",
    "\n",
    "We'll now use the Hugging Face `text-generation` task to run inference on the Llama2-7b model; we'll generate text based on an included prompt.  Notice that we've included a path to the PEFT model that we've just created.\n",
    "\n",
    "First, we'll move to the text-generation examples folder and install the requirements. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "14841b58-2697-459d-ace5-763d721468f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/ufa813565e6c2cd47e0e35a43b155dd4/Gaudi-tutorials/PyTorch/Single_card_tutorials/optimum-habana/examples/text-generation\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip available: \u001b[0m\u001b[31;49m22.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.3.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython3.10 -m pip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "%cd ~/Gaudi-tutorials/PyTorch/Single_card_tutorials/optimum-habana/examples/text-generation\n",
    "!pip install -q -r requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03479984-e872-48df-b106-cb9b1dd445d3",
   "metadata": {},
   "source": [
    "You will see that we are now running inference with the `run_generation.py` task and we are including the PEFT model that we Fine Tuned in the steps above. \n",
    "\n",
    "```\n",
    "--prompt \"I am a dog. Please help me plan a surprise birthday party for my human, including fun activities, games and decorations. And don't forget to order a big bone-shaped cake for me to share with my fur friends!\" \\\n",
    "--peft_model /root/Gaudi-tutorials/PyTorch/Single_card_tutorials/optimum-habana/examples/language-modeling/model_lora_llama_single\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e32f89a0-4bdc-4ca4-9855-88060b7f140e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter a prompt for text generation:  we are in super computing conference\n"
     ]
    }
   ],
   "source": [
    "# Use the prompt above to compare between the PEFT and non-PEFT examples\n",
    "prompt = input(\"Enter a prompt for text generation: \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d466f4a2-3607-4e8f-8b2d-1aefcc7d81aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "python3 run_generation.py  --model_name_or_path huggyllama/llama-7b --batch_size 1 --do_sample --max_new_tokens 300 --n_iterations 4       --use_hpu_graphs --use_kv_cache --bf16 --prompt \"we are in super computing conference\" \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ufa813565e6c2cd47e0e35a43b155dd4/.local/lib/python3.10/site-packages/transformers/utils/generic.py:462: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
      "  _torch_pytree._register_pytree_node(\n",
      "/home/ufa813565e6c2cd47e0e35a43b155dd4/.local/lib/python3.10/site-packages/transformers/utils/generic.py:319: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
      "  _torch_pytree._register_pytree_node(\n",
      "/home/ufa813565e6c2cd47e0e35a43b155dd4/.local/lib/python3.10/site-packages/transformers/utils/generic.py:319: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
      "  _torch_pytree._register_pytree_node(\n",
      "Fetching 2 files: 100%|██████████| 2/2 [00:00<00:00, 35246.25it/s]\n",
      "Fetching 2 files: 100%|██████████| 2/2 [00:00<00:00, 3990.77it/s]\n",
      "11/14/2024 05:45:09 - INFO - __main__ - Single-device run.\n",
      "/home/ufa813565e6c2cd47e0e35a43b155dd4/.local/lib/python3.10/site-packages/huggingface_hub/file_download.py:797: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:37<00:00, 18.86s/it]\n",
      "============================= HABANA PT BRIDGE CONFIGURATION =========================== \n",
      " PT_HPU_LAZY_MODE = 1\n",
      " PT_RECIPE_CACHE_PATH = \n",
      " PT_CACHE_FOLDER_DELETE = 0\n",
      " PT_HPU_RECIPE_CACHE_CONFIG = \n",
      " PT_HPU_MAX_COMPOUND_OP_SIZE = 9223372036854775807\n",
      " PT_HPU_LAZY_ACC_PAR_MODE = 1\n",
      " PT_HPU_ENABLE_REFINE_DYNAMIC_SHAPES = 0\n",
      "---------------------------: System Configuration :---------------------------\n",
      "Num CPU Cores : 10\n",
      "CPU RAM       : 100936596 KB\n",
      "------------------------------------------------------------------------------\n",
      "11/14/2024 05:45:59 - INFO - __main__ - Args: Namespace(device='hpu', model_name_or_path='huggyllama/llama-7b', bf16=True, max_new_tokens=300, max_input_tokens=0, batch_size=1, warmup=3, n_iterations=4, local_rank=0, use_kv_cache=True, use_hpu_graphs=True, dataset_name=None, column_name=None, do_sample=True, num_beams=1, trim_logits=False, seed=27, profiling_warmup_steps=0, profiling_steps=0, prompt=['we are in super computing conference'], bad_words=None, force_words=None, peft_model=None, num_return_sequences=1, token=None, model_revision='main', attn_softmax_bf16=False, output_dir=None, bucket_size=-1, bucket_internal=False, dataset_max_samples=-1, limit_hpu_graphs=False, reuse_cache=False, verbose_workers=False, simulate_dyn_prompt=None, reduce_recompile=False, fp8=False, use_flash_attention=False, flash_attention_recompute=False, flash_attention_causal_mask=False, book_source=False, torch_compile=False, temperature=1.0, top_p=1.0, const_serialization_path=None, disk_offload=False, quant_config='', world_size=0, global_rank=0)\n",
      "11/14/2024 05:45:59 - INFO - __main__ - device: hpu, n_hpu: 0, bf16: True\n",
      "11/14/2024 05:45:59 - INFO - __main__ - Model initialization took 51.317s\n",
      "11/14/2024 05:45:59 - INFO - __main__ - Graph compilation...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warming up\n",
      "Warming up\n",
      "Warming up\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "11/14/2024 05:46:07 - INFO - __main__ - Running generate...\n"
     ]
    }
   ],
   "source": [
    "cmd = f'python3 run_generation.py  --model_name_or_path huggyllama/llama-7b --batch_size 1 --do_sample --max_new_tokens 300 --n_iterations 4 \\\n",
    "      --use_hpu_graphs --use_kv_cache --bf16 --prompt \"{prompt}\" '\n",
    "      \n",
    "print(cmd)\n",
    "import os\n",
    "os.system(cmd)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d269a38c-eeab-49d8-b12a-73599636e445",
   "metadata": {},
   "source": [
    "##### Comparison without PEFT and LoRA\n",
    "In this example, we're simply running the Llama2 7B model **without** including the PEFT fine tuned model, so the you are losing the additional detail that is brought to the model, and the results have signficantly less information and fidelity compared to the last model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0a7a9804-cf3e-4bd3-adba-b8d39a9d55a9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "python3 run_generation.py  --model_name_or_path meta-llama/Llama-2-7b-hf --batch_size 1 --do_sample --max_new_tokens 300 --n_iterations 4       --use_hpu_graphs --use_kv_cache --bf16 --prompt \"a red cat with red hat\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ufa813565e6c2cd47e0e35a43b155dd4/.local/lib/python3.10/site-packages/transformers/utils/generic.py:462: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
      "  _torch_pytree._register_pytree_node(\n",
      "/home/ufa813565e6c2cd47e0e35a43b155dd4/.local/lib/python3.10/site-packages/transformers/utils/generic.py:319: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
      "  _torch_pytree._register_pytree_node(\n",
      "/home/ufa813565e6c2cd47e0e35a43b155dd4/.local/lib/python3.10/site-packages/transformers/utils/generic.py:319: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
      "  _torch_pytree._register_pytree_node(\n",
      "Fetching 2 files:   0%|          | 0/2 [00:00<?, ?it/s]\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/ufa813565e6c2cd47e0e35a43b155dd4/.local/lib/python3.10/site-packages/huggingface_hub/utils/_http.py\", line 406, in hf_raise_for_status\n",
      "    response.raise_for_status()\n",
      "  File \"/home/ufa813565e6c2cd47e0e35a43b155dd4/.local/lib/python3.10/site-packages/requests/models.py\", line 1024, in raise_for_status\n",
      "    raise HTTPError(http_error_msg, response=self)\n",
      "requests.exceptions.HTTPError: 401 Client Error: Unauthorized for url: https://huggingface.co/meta-llama/Llama-2-7b-hf/resolve/01c7f73d771dfac7d292323805ebc428287df4f9/model-00001-of-00002.safetensors\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/ufa813565e6c2cd47e0e35a43b155dd4/Gaudi-tutorials/PyTorch/Single_card_tutorials/optimum-habana/examples/text-generation/run_generation.py\", line 626, in <module>\n",
      "    main()\n",
      "  File \"/home/ufa813565e6c2cd47e0e35a43b155dd4/Gaudi-tutorials/PyTorch/Single_card_tutorials/optimum-habana/examples/text-generation/run_generation.py\", line 278, in main\n",
      "    model, tokenizer, generation_config = initialize_model(args, logger)\n",
      "  File \"/home/ufa813565e6c2cd47e0e35a43b155dd4/Gaudi-tutorials/PyTorch/Single_card_tutorials/optimum-habana/examples/text-generation/utils.py\", line 362, in initialize_model\n",
      "    get_repo_root(args.model_name_or_path, local_rank=args.local_rank, token=args.token)\n",
      "  File \"/home/ufa813565e6c2cd47e0e35a43b155dd4/.local/lib/python3.10/site-packages/optimum/habana/checkpoint_utils.py\", line 34, in get_repo_root\n",
      "    cache_dir = snapshot_download(\n",
      "  File \"/home/ufa813565e6c2cd47e0e35a43b155dd4/.local/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py\", line 114, in _inner_fn\n",
      "    return fn(*args, **kwargs)\n",
      "  File \"/home/ufa813565e6c2cd47e0e35a43b155dd4/.local/lib/python3.10/site-packages/huggingface_hub/_snapshot_download.py\", line 293, in snapshot_download\n",
      "    thread_map(\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/tqdm/contrib/concurrent.py\", line 69, in thread_map\n",
      "    return _executor_map(ThreadPoolExecutor, fn, *iterables, **tqdm_kwargs)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/tqdm/contrib/concurrent.py\", line 51, in _executor_map\n",
      "    return list(tqdm_class(ex.map(fn, *iterables, chunksize=chunksize), **kwargs))\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/tqdm/std.py\", line 1181, in __iter__\n",
      "    for obj in iterable:\n",
      "  File \"/usr/lib/python3.10/concurrent/futures/_base.py\", line 621, in result_iterator\n",
      "    yield _result_or_cancel(fs.pop())\n",
      "  File \"/usr/lib/python3.10/concurrent/futures/_base.py\", line 319, in _result_or_cancel\n",
      "    return fut.result(timeout)\n",
      "  File \"/usr/lib/python3.10/concurrent/futures/_base.py\", line 458, in result\n",
      "    return self.__get_result()\n",
      "  File \"/usr/lib/python3.10/concurrent/futures/_base.py\", line 403, in __get_result\n",
      "    raise self._exception\n",
      "  File \"/usr/lib/python3.10/concurrent/futures/thread.py\", line 58, in run\n",
      "    result = self.fn(*self.args, **self.kwargs)\n",
      "  File \"/home/ufa813565e6c2cd47e0e35a43b155dd4/.local/lib/python3.10/site-packages/huggingface_hub/_snapshot_download.py\", line 267, in _inner_hf_hub_download\n",
      "    return hf_hub_download(\n",
      "  File \"/home/ufa813565e6c2cd47e0e35a43b155dd4/.local/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py\", line 114, in _inner_fn\n",
      "    return fn(*args, **kwargs)\n",
      "  File \"/home/ufa813565e6c2cd47e0e35a43b155dd4/.local/lib/python3.10/site-packages/huggingface_hub/file_download.py\", line 862, in hf_hub_download\n",
      "    return _hf_hub_download_to_cache_dir(\n",
      "  File \"/home/ufa813565e6c2cd47e0e35a43b155dd4/.local/lib/python3.10/site-packages/huggingface_hub/file_download.py\", line 969, in _hf_hub_download_to_cache_dir\n",
      "    _raise_on_head_call_error(head_call_error, force_download, local_files_only)\n",
      "  File \"/home/ufa813565e6c2cd47e0e35a43b155dd4/.local/lib/python3.10/site-packages/huggingface_hub/file_download.py\", line 1484, in _raise_on_head_call_error\n",
      "    raise head_call_error\n",
      "  File \"/home/ufa813565e6c2cd47e0e35a43b155dd4/.local/lib/python3.10/site-packages/huggingface_hub/file_download.py\", line 1376, in _get_metadata_or_catch_error\n",
      "    metadata = get_hf_file_metadata(\n",
      "  File \"/home/ufa813565e6c2cd47e0e35a43b155dd4/.local/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py\", line 114, in _inner_fn\n",
      "    return fn(*args, **kwargs)\n",
      "  File \"/home/ufa813565e6c2cd47e0e35a43b155dd4/.local/lib/python3.10/site-packages/huggingface_hub/file_download.py\", line 1296, in get_hf_file_metadata\n",
      "    r = _request_wrapper(\n",
      "  File \"/home/ufa813565e6c2cd47e0e35a43b155dd4/.local/lib/python3.10/site-packages/huggingface_hub/file_download.py\", line 277, in _request_wrapper\n",
      "    response = _request_wrapper(\n",
      "  File \"/home/ufa813565e6c2cd47e0e35a43b155dd4/.local/lib/python3.10/site-packages/huggingface_hub/file_download.py\", line 301, in _request_wrapper\n",
      "    hf_raise_for_status(response)\n",
      "  File \"/home/ufa813565e6c2cd47e0e35a43b155dd4/.local/lib/python3.10/site-packages/huggingface_hub/utils/_http.py\", line 423, in hf_raise_for_status\n",
      "    raise _format(GatedRepoError, message, response) from e\n",
      "huggingface_hub.errors.GatedRepoError: 401 Client Error. (Request ID: Root=1-67358d26-45cf88dc3f7777a710af6121;cb895a26-e9d7-4085-892d-bbe8aa7de31e)\n",
      "\n",
      "Cannot access gated repo for url https://huggingface.co/meta-llama/Llama-2-7b-hf/resolve/01c7f73d771dfac7d292323805ebc428287df4f9/model-00001-of-00002.safetensors.\n",
      "Access to model meta-llama/Llama-2-7b-hf is restricted. You must have access to it and be authenticated to access it. Please log in.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "256"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cmd = f'python3 run_generation.py  --model_name_or_path huggyllama/llama-7b --batch_size 1 --do_sample --max_new_tokens 300 --n_iterations 4 \\\n",
    "      --use_hpu_graphs --use_kv_cache --bf16 --prompt \"{prompt}\"'\n",
    "print(cmd)\n",
    "import os\n",
    "os.system(cmd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a34e1747-57ea-44aa-a564-4af8d303d72a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Please be sure to run this exit command to ensure that the resorces running on Intel Gaudi are released \n",
    "exit()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
